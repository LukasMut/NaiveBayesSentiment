{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:03:16.502082Z",
     "start_time": "2019-05-15T13:03:16.492055Z"
    }
   },
   "source": [
    "### Naive Bayes in movie review data\n",
    " \n",
    " * Pang, B., Lee, L., & Vaithyanathan, S. (2002, July). [Thumbs up?: sentiment classification using machine learning techniques.](http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf) In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10 (pp. 79-86). Association for Computational Linguistics.\n",
    " \n",
    " * You can download the data from [this website](http://www.cs.cornell.edu/people/pabo/movie-review-data/) (There are different versions, let's download the 1.1 version of the polarity dataset (`polarity dataset v1.1 (2.2Mb) (includes README.1.1):...`))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:12.425840Z",
     "start_time": "2019-05-15T13:07:10.860661Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:12.436855Z",
     "start_time": "2019-05-15T13:07:12.431842Z"
    }
   },
   "outputs": [],
   "source": [
    "# download the dataset (positive and negative tokens) first\n",
    "cwd = os.getcwd()\n",
    "pos_path = cwd + '/tokens/pos/'\n",
    "neg_path = cwd + '/tokens/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:20.920949Z",
     "start_time": "2019-05-15T13:07:12.443881Z"
    }
   },
   "outputs": [],
   "source": [
    "def filetowordlist(path, sfx):\n",
    "    words = []\n",
    "    for item in os.listdir(path):\n",
    "        if sfx in item:\n",
    "            f=open(path + item, encoding=\"iso8859-1\")\n",
    "            lines = [line.strip() for line in f]\n",
    "            f.close()\n",
    "            wordsinfile = \"\"\n",
    "            for l in lines:\n",
    "                wordsinfile = wordsinfile + l + \" \"\n",
    "            \n",
    "            words.append(wordsinfile)\n",
    "    return words\n",
    "\n",
    "possents_all = np.array(filetowordlist(pos_path, \".txt\"))\n",
    "negsents_all = np.array(filetowordlist(neg_path, \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:20.935990Z",
     "start_time": "2019-05-15T13:07:20.922958Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(sents: list):\n",
    "    \"\"\"\n",
    "    This function computes preprocessing. The prefix NOT is prepended to every word \n",
    "    after a token of logical negation (i.e., nâ€™t, not, no, never) until the next punctuation mark.\n",
    "    \n",
    "    Newly formed 'words' like NOT like, NOT recommend will thus occur more often\n",
    "    in negative document and act as cues for negative sentiment, while words like\n",
    "    NOT bored, NOT dismiss will acquire positive associations.\n",
    "    \n",
    "    Non-alphanumerical characters are removed and any digit will be converted to #.\n",
    "    \"\"\"\n",
    "    vocab = dict()\n",
    "    new_sents = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        \n",
    "        new_sent = []\n",
    "        sent = sent.split()\n",
    "        idx = 0\n",
    "        \n",
    "        for i, word in enumerate(sent):\n",
    "            \n",
    "            # list comprehension is computationally more efficient (i.e., faster) than regex\n",
    "            word = re.sub(\"[^a-zA-Z0-9]+\", \"\", word)\n",
    "            # word = ''.join([char for char in word if char.isalnum()]) \n",
    "            word = re.sub(\"[0-9]+\", \"#\", word)\n",
    "            \n",
    "            if i > 0:\n",
    "                \n",
    "                if (re.search(r\"(nt|n't|\\b[nN]ot\\b|\\b[nN]o\\b|\\b[nN]ever\\b)\", sent[idx-1]) \n",
    "                    and not re.search(\"^[\\.,:;?!]\", sent[i]) \n",
    "                    and not re.search(\"[\\.,:;?!]$\", sent[i-1])):\n",
    "                    \n",
    "                    word = \"NOT_\" + word\n",
    "                    \n",
    "                else:\n",
    "                    idx += int(abs(i-idx) + 1)\n",
    "            else:\n",
    "                idx += 1\n",
    "            \n",
    "            if len(word) > 0:\n",
    "                \n",
    "                new_sent.append(word)\n",
    "\n",
    "                if word in vocab:\n",
    "                    vocab[word] += 1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "            \n",
    "        new_sents.append(' '.join(new_sent))\n",
    "        \n",
    "    return vocab, new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:20.946017Z",
     "start_time": "2019-05-15T13:07:20.937996Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary(pos_sents, neg_sents, threshold = 3):\n",
    "    \n",
    "    vocab_pos, pos_sents = text_preprocessing(pos_sents)\n",
    "    vocab_neg, neg_sents = text_preprocessing(neg_sents)\n",
    "    \n",
    "    # concatenate the vocabs\n",
    "    for word, freq in vocab_neg.items():\n",
    "        if word in vocab_pos:\n",
    "            vocab_pos[word] += freq\n",
    "        else:\n",
    "            vocab_pos[word] = freq\n",
    "    \n",
    "    # only keep words that occur more often than a specific threshold\n",
    "    vocab = [word for word, freq in vocab_pos.items() if freq >= threshold]\n",
    "    \n",
    "    return vocab, pos_sents, neg_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:26.186235Z",
     "start_time": "2019-05-15T13:07:20.949025Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab, pos_sents, neg_sents = extract_vocabulary(possents_all, negsents_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each document will be represented as a unigram bag-of-words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:26.217521Z",
     "start_time": "2019-05-15T13:07:26.186235Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize(pos_sents: list, neg_sents: list, vocab: list, n_best_factor = 1):\n",
    "    \"\"\"\n",
    "    Word unigram document representation using tf-idf weighting.\n",
    "    This function takes a precomputed vocabulary, that is passed to the tf-idf vectorizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.concatenate((pos_sents, neg_sents))\n",
    "    y = np.concatenate((np.ones(len(pos_sents), dtype = int), np.zeros(len(neg_sents), dtype = int)))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(encoding = 'utf-8', lowercase = False, ngram_range = (1, 1), \n",
    "                                 analyzer = 'word', norm = 'l2', use_idf = True, smooth_idf = True,\n",
    "                                 sublinear_tf = True, vocabulary = vocab)\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "        \n",
    "    #n_best = int(len(vectorizer.idf_) * n_best_factor)\n",
    "    #idx_best = np.argsort(vectorizer.idf_)[:n_best]\n",
    "        \n",
    "    #X_train = X_train[:, idx_best]\n",
    "    #X_test = X_test[:, idx_best]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:27.070494Z",
     "start_time": "2019-05-15T13:07:26.217521Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = vectorize(pos_sents, neg_sents, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing using sklearn's Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:27.087507Z",
     "start_time": "2019-05-15T13:07:27.077482Z"
    }
   },
   "outputs": [],
   "source": [
    "MNB = MultinomialNB(fit_prior = True)\n",
    "MNB.fit(X_train, y_train)\n",
    "y_pred = MNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T13:07:27.100543Z",
     "start_time": "2019-05-15T13:07:27.089513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
